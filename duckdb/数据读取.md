# Duckdb
Duckdb 既支持文件 database, 也支持 内存 database. 当操作的数据大于内存上限时，会使用 disk
# 数据读取
DuckDB 支持读取多种数据源，比如 csv, jsons 等等
```js
create table data (i integer, j interger)

// 通过 insert 插入数据
insert into data values (1, 1), (2, 2)
insert into data (i) values (3), (4)

// load csv
// select from csv file directly
select * from 'test.csv'

// copy from csv
// 将 csv 的内容直接 copy 到 data 里
copy data from 'test.csv'

// select from read_csv method to pass along options
// 常用的参数就有 header(bool, 指定是否有 header 行), skip(int, 指定需要跳过的行) 等
// preserve_insertion_order，默认值为 true, 用来指定是否需要保持顺序
select * from read_csv('test.csv', header=false)

// create table from csv
create table data as select * from 'test.csv'
create table data as from 'test.csv' // 简写

// 导出 selection 到 csv
copy (select * from data) to 'data.csv' with (HEADER, DELIMITER '|')
// 导出 table 到 csv
copy data to 'data.csv' with (HEADER, DELIMITER '|')

// 对于多文件
// Wildcard	Description
// *	Matches any number of any characters (including none)
// **	Matches any number of subdirectories (including none)
// ?	Matches any single character
// [abc]	Matches one character given in the bracket
// [a-z]	Matches one character from the range given in the bracket
select * from 'dir/*/*/*.csv'
// 当读取多个文件时，文件之间的 union 方式有两种，一种时 union_by_position, 一种是 union_by_name
select * from read_csv(['data1.csv', 'data2.csv'], union_by_name=true, file_name=true)
select * from read_csv(['dir/*.csv', 'data2.csv'])

// parquet file and json file has similar function, read_json, read_parquet

// Appender(C, C++, Java, GO, Rust) 可以用来不使用 sql 而加载大量的数据，高效快捷
// 
```
在 laod csv 时，需要注意的是 duckdb load faulty csv 时，容忍度比较高，很多 case 都不会报错，而是 load 成功，可以使用 pandas 来先校验一下 csv 文件

# 数据分割(data partition)
Duckdb 采用 hive partition 的 策略, 将大数据 table split into multiple files based on partition keys.  
需要注意的是 PARTITION_BY 后面的 partion key 不可以是 expression
```js
// 将 selection 按照 partition key 导出
// OVERWRITE_OR_IGNORE 允许 overwrite
COPY (SELECT *, year(timestamp) AS year, month(timestamp) AS month FROM services)
TO 'orders' (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE);

// 还可以指定 compression, 以及文件后缀
COPY orders TO 'orders' (FORMAT CSV, PARTITION_BY (year, month), COMPRESSION GZIP, FILE_EXTENSION 'csv.gz');
// 还可以指定生成的文件名的 pattern
// i 会被替换成当前文件的 index, uuid 会被替换为一个 128bits 的 uuid
COPY orders TO 'orders'
    (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE, FILENAME_PATTERN 'orders_{i}');

// 导出的目录结构如下所示
// duckdb 在 load 多文件时，会自动去 infer 文件是否满足 hive partition 分布。 这个自动检测会去检查文件名是否满足 key=value 的 pattern
// 如果想 disable 这个功能，可以 SET hive_partitioning = false 
orders
├── year=2021
│    ├── month=1
│    │   ├── file1.parquet
│    │   └── file2.parquet
│    └── month=2
│        └── file3.parquet
└── year=2022
     ├── month=11
     │   ├── file4.parquet
     │   └── file5.parquet
     └── month=12
         └── file6.parquet

// load partioned data
// hive_partitioning = true 意味着需要将 partitioned columns 包含在 selection 中
// 对上例来说，如果为 true, year, month 两列将会被包含在 selection 里
select * from read_parquet('orders/*/*/*.parquet', hive_partitioning=true)

// 文件过滤
// 如果只需要 load 部分文件，可以直接在 sql 语句中加上 where 条件，而不需要去调整 'orders/*/*/*.parquet'
// duckdb 会自动 skip 不需要加载的文件
SELECT *
FROM read_parquet('orders/*/*/*.parquet', hive_partitioning = true)
WHERE year = 2022
  AND month = 11;
```
把数据分割成很多很小的 partition 是很昂贵的，通常每个 partition 需要有至少 100MB 大小
